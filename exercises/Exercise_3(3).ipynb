{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["# VDL Exericse 3 (Task 3.4)\n","\n","**Group Name:** ...\n","\n","\n","**Participants:**\n","\n","- Name 1 (Matrikl. Nr. 1)\n","- Name 2 (Matrikl. Nr. 2)\n","- ...\n","\n","__Complete all sub-tasks marked with ## TODO ## and submit the filled notebook on OLAT__"]},{"cell_type":"code","metadata":{"id":"XLsSEKKYHPZ5"},"source":["import os\n","import copy\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","from torch.autograd import Variable\n","from torch.optim import Adam\n","from torchvision import models, transforms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lzfJtW-AGiTB"},"source":["# Visualizing Convolutional Neural Networks\n","\n","Download a pretrained AlexNet model and plot the weights of the first convolutional layer"]},{"cell_type":"code","metadata":{"id":"2_kWw0S7GiTG"},"source":["# Download a pretrained AlexNet\n","alexnet = models.alexnet(pretrained=True).features\n","alexnet"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Filter Visualization\n","\n","In filter visualization, we will extract the weights of the filters at first Conv2d layer of a pretrained AlexNet, and show them as images using `plt`.\n","\n","_Hints_:\n","- See \"[What is a `state_dict`?](https://pytorch.org/tutorials/beginner/saving_loading_models)\" for accessing module weights.\n","- See [matplotlib subfigures](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.add_subplot) for creating image grids."],"metadata":{"id":"wV0DWYT7164c"}},{"cell_type":"code","metadata":{"id":"z0VmdlSyHAbu"},"source":["# TODO: Extract weights of the first Conv2d layer\n","w = None\n","\n","# Convert weights Tensor to numpy and reshape\n","w = w.detach().numpy().transpose(0, 2, 3, 1)\n","print(w.shape)  # Should be (64, 11, 11, 3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VYo3i1E5ISW2"},"source":["# TODO: Show extracted weights as an 8x8 grid of images\n","# Hint: Normalize the weights in [0, 1] before plt.imshow\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Activation Map Visualization"],"metadata":{"id":"4RHF7LFe9uVP"}},{"cell_type":"code","source":["# Download a pretrained VGG16\n","vgg16 = models.vgg16(pretrained=True).features\n","vgg16"],"metadata":{"id":"rsdnjTYa13j-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conv_layers = []\n","\n","# TODO: Append Conv2D layers in vgg16 to list\n","\n","print(len(conv_layers))"],"metadata":{"id":"3W_wBGlqAI3P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: Download an image of your choice from the internet using the `wget` command\n","# command and open it as a numpy array using PIL.Image.\n","\n","\n","# TODO: Open the downloaded image using PIL.Image and convert to numpy array\n","im_arr = None\n","plt.imshow(im_arr)"],"metadata":{"id":"89Vv-XezAF0J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose([transforms.ToPILImage(),\n","                                transforms.Resize((224, 224)),\n","                                transforms.ToTensor()])\n","\n","im = transform(im_arr).unsqueeze_(0)\n","print(im.size())"],"metadata":{"id":"mB7txz7IA161"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activations = []\n","\n","# TODO: Forward pass 'im' through each of the conv_layers and save output in list\n","\n","print(len(activations))  # Should be same as number of conv_layers"],"metadata":{"id":"6MwhakMTDn0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize the activation maps\n","rows, columns = len(activations), 16\n","plt.figure(figsize=[columns, rows])\n","index = 1\n","for layer in range(rows):\n","  maps = activations[layer][0].data\n","  for i, act_map in enumerate(maps):\n","      if i >= columns:\n","        break\n","\n","      plt.subplot(rows, columns, index)\n","      plt.imshow(act_map, cmap='gray')\n","      plt.axis('off')\n","      index += 1\n","\n","plt.show()"],"metadata":{"id":"jSBwUUUP_X-3"},"execution_count":null,"outputs":[]}]}