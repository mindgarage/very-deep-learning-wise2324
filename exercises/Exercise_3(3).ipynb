{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["# Programming Exercise 3(3): Visualizing Convolutional Neural Networks\n","\n","## Very Deep Learning (VDL) - Winter Semester 2023/24\n","\n","---\n","\n","### Group Details:\n","\n","- **Group Name:** \\[Enter OLAT Group Name Here\\]\n","\n","### Members:\n","\n","- \\[Participant 1 Name\\], \\[Matrikel-Nr 1\\]\n","- \\[Participant 2 Name\\], \\[Matrikel-Nr 1\\]\n","- ...\n","\n","---\n","\n","**Instructions**: The tasks in this notebook are a part of Sheet 3. Look for `TODO` tags throughout the notebook and complete the sections with missing code. Once done, ensure all outputs are visible and correctly displayed. Save your notebook and submit the `.ipynb` file together with the exercise sheet PDF in a single ZIP file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XLsSEKKYHPZ5"},"outputs":[],"source":["import os\n","import copy\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","from torch.autograd import Variable\n","from torch.optim import Adam\n","from torchvision import models, transforms"]},{"cell_type":"markdown","metadata":{"id":"lzfJtW-AGiTB"},"source":["Download a pretrained AlexNet model and plot the weights of the first convolutional layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_kWw0S7GiTG"},"outputs":[],"source":["# Download a pretrained AlexNet\n","alexnet = models.alexnet(pretrained=True).features\n","alexnet"]},{"cell_type":"markdown","metadata":{"id":"wV0DWYT7164c"},"source":["## Filter Visualization\n","\n","In filter visualization, we will extract the weights of the filters at first Conv2d layer of a pretrained AlexNet, and show them as images using `plt`.\n","\n","_Hints_:\n","- See \"[What is a `state_dict`?](https://pytorch.org/tutorials/beginner/saving_loading_models)\" for accessing module weights.\n","- See [matplotlib subfigures](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.add_subplot) for creating image grids."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0VmdlSyHAbu"},"outputs":[],"source":["# TODO: Extract weights of the first Conv2d layer\n","w = None\n","\n","# Convert weights Tensor to numpy and reshape\n","w = w.detach().numpy().transpose(0, 2, 3, 1)\n","print(w.shape)  # Should be (64, 11, 11, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYo3i1E5ISW2"},"outputs":[],"source":["# TODO: Show extracted weights as an 8x8 grid of images\n","# Hint: Normalize the weights in [0, 1] before plt.imshow\n"]},{"cell_type":"markdown","metadata":{"id":"4RHF7LFe9uVP"},"source":["## Activation Map Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsdnjTYa13j-"},"outputs":[],"source":["# Download a pretrained VGG16\n","vgg16 = models.vgg16(pretrained=True).features\n","vgg16"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3W_wBGlqAI3P"},"outputs":[],"source":["conv_layers = []\n","\n","# TODO: Append Conv2D layers in vgg16 to list\n","\n","print(len(conv_layers))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"89Vv-XezAF0J"},"outputs":[],"source":["# TODO: Download an image of your choice from the internet using the `wget` command\n","# command and open it as a numpy array using PIL.Image.\n","\n","\n","# TODO: Open the downloaded image using PIL.Image and convert to numpy array\n","im_arr = None\n","plt.imshow(im_arr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mB7txz7IA161"},"outputs":[],"source":["transform = transforms.Compose([transforms.ToPILImage(),\n","                                transforms.Resize((224, 224)),\n","                                transforms.ToTensor()])\n","\n","im = transform(im_arr).unsqueeze_(0)\n","print(im.size())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6MwhakMTDn0E"},"outputs":[],"source":["activations = []\n","\n","# TODO: Forward pass 'im' through each of the conv_layers and save output in list\n","\n","print(len(activations))  # Should be same as number of conv_layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jSBwUUUP_X-3"},"outputs":[],"source":["# Visualize the activation maps\n","rows, columns = len(activations), 16\n","plt.figure(figsize=[columns, rows])\n","index = 1\n","for layer in range(rows):\n","  maps = activations[layer][0].data\n","  for i, act_map in enumerate(maps):\n","      if i >= columns:\n","        break\n","\n","      plt.subplot(rows, columns, index)\n","      plt.imshow(act_map, cmap='gray')\n","      plt.axis('off')\n","      index += 1\n","\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat":4,"nbformat_minor":0}
